{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../berries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/Projects/picojam/pico/.venv/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for mnist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mnist\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mnist = load_dataset(\"mnist\").with_format(\"jax\")\n",
    "mnistData = mnist['train']\n",
    "X_img = mnistData['image']\n",
    "y = mnistData['label']\n",
    "X_img_test = mnist[\"test\"][\"image\"]\n",
    "n_test_samples = X_img_test.shape[0]\n",
    "X_test = X_img_test.reshape((n_test_samples, -1))\n",
    "y_test = mnist[\"test\"][\"label\"]\n",
    "n_samples, _, _ = X_img.shape\n",
    "X = X_img.reshape((n_samples, -1))\n",
    "n_samples, d_in = X.shape\n",
    "d_out = len(set(y.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_h_layer = 128\n",
    "d_h = 64\n",
    "n_layers = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1588132\n",
      "tuple:\n",
      "    sglu:\n",
      "        wv:\n",
      "            array shape: (784, 64)\n",
      "        wu:\n",
      "            array shape: (784, 64)\n",
      "        wo:\n",
      "            array shape: (64, 128)\n",
      "        total params: 108544\n",
      "    rmsn:\n",
      "        d:\n",
      "            128.0\n",
      "        total params: 1\n",
      "    total params: 108545\n",
      "tuple:\n",
      "    sglu:\n",
      "        wv:\n",
      "            array shape: (128, 64)\n",
      "        wu:\n",
      "            array shape: (128, 64)\n",
      "        wo:\n",
      "            array shape: (64, 10)\n",
      "        total params: 17024\n",
      "    rmsn:\n",
      "        d:\n",
      "            10.0\n",
      "        total params: 1\n",
      "    total params: 17025\n",
      "total params: 125570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "from jax.numpy import mean\n",
    "from jax import grad, jit\n",
    "from jax.tree_util import tree_map\n",
    "from pf import F, _\n",
    "# import nn\n",
    "# import importlib\n",
    "# importlib.reload(nn)\n",
    "import random_utils\n",
    "from nn import mglu_config, mglu, init_weights, fmt_weights\n",
    "\n",
    "seed = 0\n",
    "key_gen = random_utils.infinite_safe_keys(seed)\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(-1) == y).mean()\n",
    "\n",
    "\n",
    "mglu_b = F(mglu).vmap((0, None, None), 0)\n",
    "\n",
    "def sce_loss(to_logits, x, y, W):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(to_logits(x, W), y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_b_all = F(sce_loss).f(mglu, _, _, _).vmap((0, 0, None), 0)\n",
    "loss_d = loss_b_all.f(X, y, _)\n",
    "loss_b_d = loss_d >> mean\n",
    "\n",
    "lr = 0.001\n",
    "mask_fn = lambda p: tree_map(lambda x: not isinstance(x, int), p)\n",
    "# opt = optax.multi_transform({\"sgd\": optax.rmsprop(lr), \"zero\": optax.set_to_zero()}, mask_fn)\n",
    "opt = optax.masked(optax.rmsprop(lr), mask_fn)\n",
    "\n",
    "\n",
    "method = {\"type\": \"zer0\", \"std\": 0.001}\n",
    "# method = {\"type\": \"normal\", \"std\": 0.01}\n",
    "W = init_weights(next(key_gen), mglu_config(d_in, d_h_layer, d_out, d_h, n_layers, method))\n",
    "loss0 = loss_b_d(W)\n",
    "print(loss0)\n",
    "print(fmt_weights(W)[0])\n",
    "state = opt.init(W)\n",
    "\n",
    "@jit\n",
    "def update(W, opt_state):\n",
    "    grads = grad(loss_b_d)(W)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    new_W = optax.apply_updates(W, updates)\n",
    "    return new_W, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09871667 0.098 2.9003065\n",
      "0.9199167 0.923 0.66302013\n",
      "0.9577 0.95239997 0.43886253\n",
      "0.97125 0.9625 0.37138352\n",
      "0.97688335 0.96819997 0.35107404\n",
      "0.9789 0.9691 0.34038138\n",
      "0.98256665 0.9693 0.32953045\n",
      "0.9867833 0.97249997 0.31877425\n",
      "0.98845 0.97389996 0.31075525\n",
      "0.99121666 0.9736 0.3010577\n",
      "0.98918337 0.9719 0.30412602\n",
      "0.9899833 0.96999997 0.3012402\n",
      "0.9920667 0.9711 0.2938076\n",
      "0.9943 0.9741 0.2851047\n",
      "0.99365 0.97309995 0.28448004\n",
      "0.99635 0.974 0.27580982\n",
      "0.9953167 0.9738 0.27615047\n",
      "0.9942167 0.97239995 0.27758375\n",
      "0.99805003 0.9745 0.26383737\n",
      "0.99738336 0.9729 0.265202\n",
      "[0.25933945 0.24239717 0.25948963 ... 0.24418825 0.2602984  0.25811207]\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(x, y, W):\n",
    "    return accuracy(mglu(x, W), y)\n",
    "\n",
    "get_accuracy_b = F(get_accuracy).vmap(in_axes=(0, 0, None), out_axes=0)\n",
    "get_accuracy_b_d = get_accuracy_b.f(X, y, _) >> mean\n",
    "get_accuracy_b_t = get_accuracy_b.f(X_test, y_test, _) >> mean\n",
    "\n",
    "for i in range(1000):\n",
    "    W, state = update(W, state)\n",
    "    if i % 50 == 0:\n",
    "        print(get_accuracy_b_d(W), get_accuracy_b_t(W), loss_b_d(W))\n",
    "print(loss_d(W))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
