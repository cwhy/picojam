{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functools\n",
    "from typing import Callable\n",
    "from better_partial import _, partial\n",
    "from jax import vmap\n",
    "\n",
    "\n",
    "def compose(f, g):\n",
    "    return lambda *args, **kw: g(f(*args, **kw))\n",
    "\n",
    "class PointFreeFunction:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "        functools.update_wrapper(self, func)\n",
    "    \n",
    "    def f(self, *args, **kw):\n",
    "        return PointFreeFunction(partial(self.func)(*args, **kw))\n",
    "    \n",
    "    def vmap(self, *args, **kw):\n",
    "        return PointFreeFunction(vmap(self.func, *args, **kw))\n",
    "\n",
    "    def __call__(self, *args, **kw):\n",
    "        return self.func(*args, **kw)\n",
    "    \n",
    "    def __rshift__(self, other: 'PointFreeFunction' | Callable):\n",
    "        if isinstance(other, PointFreeFunction):\n",
    "            return PointFreeFunction(compose(self.func, other.func))\n",
    "        elif callable(other):\n",
    "            return PointFreeFunction(compose(self.func, other))\n",
    "        else:\n",
    "            raise TypeError(\"other must be callable or PointFreeFunction\")\n",
    "\n",
    "F = PointFreeFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.nn import sigmoid\n",
    "from jax.numpy.linalg import norm\n",
    "from jax.numpy import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def affine(x, W, b):\n",
    "    return W.T @ x + b\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "def sglu(x, wv, wu, wo):\n",
    "    v = x @ wv\n",
    "    u = x @ wu\n",
    "    return (v * u) @ wo\n",
    "\n",
    "def rmsn(x, d):\n",
    "    return  x / (norm(x)/ sqrt(d))\n",
    "\n",
    "def mglu(x, ws):\n",
    "    for w in ws:\n",
    "        sglu_w = F(sglu).f(_, **w['sglu'])\n",
    "        x = rmsn(sglu_w(x), **w['rmsn'])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_init_std = 0.01\n",
    "#\n",
    "# #W1 = init_utils.zerO_init_2D((d_in, d_h1))\n",
    "# W1 = init_utils.normal_init(next(key_gen), normal_init_std, (d_in, d_h1))\n",
    "# b1 = zeros((d_h1))\n",
    "# #W2 = init_utils.zerO_init_2D((d_h1, d_out))\n",
    "# W2 = init_utils.normal_init(next(key_gen), normal_init_std, (d_h1, d_out))\n",
    "# b2 = zeros((d_out))\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../berries\")\n",
    "import init_utils, random_utils\n",
    "from init_utils import zerO_init_2D\n",
    "from jax.random import split\n",
    "\n",
    "\n",
    "def sglu_config(d_in, d_h, d_out, init):\n",
    "    return {\n",
    "        \"wv\": {\n",
    "            \"size\": (d_in, d_h),\n",
    "            \"init\": init,\n",
    "        },\n",
    "        \"wu\": {\n",
    "            \"size\": (d_in, d_h),\n",
    "            \"init\": init,\n",
    "        },\n",
    "        \"wo\": {\n",
    "            \"size\": (d_h, d_out),\n",
    "            \"init\": init,\n",
    "        },\n",
    "    }\n",
    "\n",
    "def rmsn_config(d_out):\n",
    "    return {\n",
    "        \"d\": {\n",
    "            \"const\": float(d_out),\n",
    "        }\n",
    "    }\n",
    "\n",
    "def mglu_layer_config(d_in, d_h, d_out, init):\n",
    "    return {\n",
    "        \"sglu\": sglu_config(d_in, d_h, d_out, init),\n",
    "        \"rmsn\": rmsn_config(d_out),\n",
    "    }\n",
    "\n",
    "\n",
    "def mglu_config(d_in, d_h_layer, d_out, d_h, n_layers, init):\n",
    "    return tuple([\n",
    "        mglu_layer_config(d_in, d_h, d_h_layer, init),\n",
    "        *[mglu_layer_config(d_h_layer, d_h, d_h_layer, init)] * (n_layers - 1),\n",
    "        mglu_layer_config(d_h_layer, d_h, d_out, init),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "seed = 0\n",
    "key_gen = random_utils.infinite_safe_keys(seed)\n",
    "\n",
    "\n",
    "def init_weight(key, init, size):\n",
    "    if init[\"type\"] == \"normal\":\n",
    "        return init_utils.normal_init(key, init[\"std\"], size)\n",
    "    elif init[\"type\"] == \"zer0\":\n",
    "        return zerO_init_2D(size)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown init type: {init['type']}\")\n",
    "\n",
    "\n",
    "def init_weights(key, configs):\n",
    "    if isinstance(configs, tuple):\n",
    "        keys = key.split(len(configs))\n",
    "        return tuple(init_weights(k, c) for c, k in zip(configs, keys))\n",
    "    elif isinstance(configs, dict):\n",
    "        if \"const\" in configs:\n",
    "            return configs[\"const\"]\n",
    "        if \"init\" in configs:\n",
    "            return init_weight(key, **configs)\n",
    "        else:\n",
    "            keys = key.split(len(configs))\n",
    "            return {name: init_weights(k, config) for (name, config), k in zip(configs.items(), keys)}\n",
    "\n",
    "\n",
    "def fmt_weights(weights, indent=0):\n",
    "    out_str = \"\"\n",
    "    out_n_params = 0\n",
    "    if isinstance(weights, tuple):\n",
    "        for w in weights:\n",
    "            sub_out_str, sub_n_params = fmt_weights(w, indent + 4)\n",
    "            out_str += f\"{' ' * indent}tuple:\\n{sub_out_str}\"\n",
    "            out_n_params += sub_n_params\n",
    "        out_str += f\"{' ' * indent}total params: {out_n_params}\\n\"\n",
    "    elif isinstance(weights, dict):\n",
    "        for name, w in weights.items():\n",
    "            sub_out_str, sub_n_params = fmt_weights(w, indent + 4)\n",
    "            out_str += f\"{' ' * indent}{name}:\\n{sub_out_str}\"\n",
    "            out_n_params += sub_n_params\n",
    "        out_str += f\"{' ' * indent}total params: {out_n_params}\\n\"\n",
    "    else:\n",
    "        if hasattr(weights, 'shape'):\n",
    "            out_n_params += weights.size\n",
    "            out_str += f\"{' ' * indent}array shape: {weights.shape}\\n\"\n",
    "        else:\n",
    "            out_n_params += 1\n",
    "            out_str += f\"{' ' * indent}{weights}\\n\"\n",
    "    return out_str, out_n_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/Projects/picojam/pico/.venv/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for mnist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mnist\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mnist = load_dataset(\"mnist\").with_format(\"jax\")\n",
    "mnistData = mnist['train']\n",
    "X_img = mnistData['image']\n",
    "y = mnistData['label']\n",
    "X_img_test = mnist[\"test\"][\"image\"]\n",
    "n_test_samples = X_img_test.shape[0]\n",
    "X_test = X_img_test.reshape((n_test_samples, -1))\n",
    "y_test = mnist[\"test\"][\"label\"]\n",
    "n_samples, _, _ = X_img.shape\n",
    "X = X_img.reshape((n_samples, -1))\n",
    "n_samples, d_in = X.shape\n",
    "d_out = len(set(y.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_h_layer = 128\n",
    "d_h = 64\n",
    "n_layers = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1588132\n",
      "tuple:\n",
      "    sglu:\n",
      "        wv:\n",
      "            array shape: (784, 64)\n",
      "        wu:\n",
      "            array shape: (784, 64)\n",
      "        wo:\n",
      "            array shape: (64, 128)\n",
      "        total params: 108544\n",
      "    rmsn:\n",
      "        d:\n",
      "            128.0\n",
      "        total params: 1\n",
      "    total params: 108545\n",
      "tuple:\n",
      "    sglu:\n",
      "        wv:\n",
      "            array shape: (128, 64)\n",
      "        wu:\n",
      "            array shape: (128, 64)\n",
      "        wo:\n",
      "            array shape: (64, 10)\n",
      "        total params: 17024\n",
      "    rmsn:\n",
      "        d:\n",
      "            10.0\n",
      "        total params: 1\n",
      "    total params: 17025\n",
      "total params: 125570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import optax\n",
    "from jax.numpy import mean\n",
    "from jax import grad, jit\n",
    "from jax.tree_util import tree_map\n",
    "from better_partial import _\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(-1) == y).mean()\n",
    "\n",
    "\n",
    "mglu_b = F(mglu).vmap((0, None, None), 0)\n",
    "\n",
    "def sce_loss(to_logits, x, y, W):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(to_logits(x, W), y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_b_all = F(sce_loss).f(mglu, _, _, _).vmap((0, 0, None), 0)\n",
    "loss_d = loss_b_all.f(X, y, _)\n",
    "loss_b_d = loss_d >> mean\n",
    "\n",
    "lr = 0.001\n",
    "mask_fn = lambda p: tree_map(lambda x: not isinstance(x, int), p)\n",
    "# opt = optax.multi_transform({\"sgd\": optax.rmsprop(lr), \"zero\": optax.set_to_zero()}, mask_fn)\n",
    "opt = optax.masked(optax.rmsprop(lr), mask_fn)\n",
    "\n",
    "\n",
    "method = {\"type\": \"zer0\", \"std\": 0.001}\n",
    "# method = {\"type\": \"normal\", \"std\": 0.01}\n",
    "W = init_weights(next(key_gen), mglu_config(d_in, d_h_layer, d_out, d_h, n_layers, method))\n",
    "loss0 = loss_b_d(W)\n",
    "print(loss0)\n",
    "print(fmt_weights(W)[0])\n",
    "state = opt.init(W)\n",
    "\n",
    "@jit\n",
    "def update(W, opt_state):\n",
    "    grads = grad(loss_b_d)(W)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    new_W = optax.apply_updates(W, updates)\n",
    "    return new_W, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09871667 0.098 2.9003065\n",
      "0.9199167 0.923 0.66302013\n",
      "0.9577 0.95239997 0.43886253\n",
      "0.97125 0.9625 0.37138352\n",
      "0.97688335 0.96819997 0.35107404\n",
      "0.9789 0.9691 0.34038138\n",
      "0.98256665 0.9693 0.32953045\n",
      "0.9867833 0.97249997 0.31877425\n",
      "0.98845 0.97389996 0.31075525\n",
      "0.99121666 0.9736 0.3010577\n",
      "0.98918337 0.9719 0.30412602\n",
      "0.9899833 0.96999997 0.3012402\n",
      "0.9920667 0.9711 0.2938076\n",
      "0.9943 0.9741 0.2851047\n",
      "0.99365 0.97309995 0.28448004\n",
      "0.99635 0.974 0.27580982\n",
      "0.9953167 0.9738 0.27615047\n",
      "0.9942167 0.97239995 0.27758375\n",
      "0.99805003 0.9745 0.26383737\n",
      "0.99738336 0.9729 0.265202\n",
      "[0.25933945 0.24239717 0.25948963 ... 0.24418825 0.2602984  0.25811207]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_accuracy(x, y, W):\n",
    "    return accuracy(mglu(x, W), y)\n",
    "\n",
    "get_accuracy_b = F(get_accuracy).vmap(in_axes=(0, 0, None), out_axes=0)\n",
    "get_accuracy_b_d = get_accuracy_b.f(X, y, _) >> mean\n",
    "get_accuracy_b_t = get_accuracy_b.f(X_test, y_test, _) >> mean\n",
    "\n",
    "for i in range(1000):\n",
    "    W, state = update(W, state)\n",
    "    if i % 50 == 0:\n",
    "        print(get_accuracy_b_d(W), get_accuracy_b_t(W), loss_b_d(W))\n",
    "print(loss_d(W))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
